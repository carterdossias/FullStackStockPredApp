{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GETTING STOCK DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported successfully into the MySQL database.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from credentials import ipCred, usernameCred, passwordCred, databaseCred\n",
    "\n",
    "def safe_float(value):\n",
    "    \"\"\"Convert a value to float if not NaN, otherwise return None.\"\"\"\n",
    "    if pd.isnull(value):\n",
    "        return None\n",
    "    return float(value)\n",
    "\n",
    "def main():\n",
    "    # Database connection parameters\n",
    "    server = ipCred\n",
    "    username = usernameCred\n",
    "    password = passwordCred\n",
    "    database = databaseCred\n",
    "    \n",
    "    # Establish connection to MySQL\n",
    "    conn = mysql.connector.connect(\n",
    "        host=server,\n",
    "        user=username,\n",
    "        password=password,\n",
    "        database=database\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Define the ticker symbol\n",
    "    ticker_symbol = 'PN'\n",
    "    \n",
    "    # Fetch historical stock data using yfinance\n",
    "    stock = yf.Ticker(ticker_symbol)\n",
    "    hist = stock.history(period=\"max\")\n",
    "    \n",
    "    # Localize the index to UTC to bypass DST-related issues\n",
    "    if hist.index.tzinfo is None:\n",
    "        hist.index = hist.index.tz_localize('UTC', nonexistent='shift_forward', ambiguous='NaT')\n",
    "    else:\n",
    "        hist.index = hist.index.tz_convert('UTC')\n",
    "    \n",
    "    # Create table name based on ticker symbol (e.g., DIS_data)\n",
    "    table_name = f\"{ticker_symbol}_data\"\n",
    "    \n",
    "    # SQL to create table if it doesn't exist (Best practice to create table in MySQL Workbench first)\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        date DATE PRIMARY KEY,\n",
    "        open FLOAT,\n",
    "        high FLOAT,\n",
    "        low FLOAT,\n",
    "        close FLOAT,\n",
    "        volume BIGINT\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    \n",
    "    # SQL to insert data into the table with upsert functionality\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO {table_name} (date, open, high, low, close, volume)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    ON DUPLICATE KEY UPDATE\n",
    "        open = VALUES(open),\n",
    "        high = VALUES(high),\n",
    "        low = VALUES(low),\n",
    "        close = VALUES(close),\n",
    "        volume = VALUES(volume)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterate over DataFrame rows and insert into the table\n",
    "    for index, row in hist.iterrows():\n",
    "        data_tuple = (\n",
    "            index.date(), \n",
    "            safe_float(row.get('Open')), \n",
    "            safe_float(row.get('High')), \n",
    "            safe_float(row.get('Low')), \n",
    "            safe_float(row.get('Close')), \n",
    "            int(row.get('Volume')) if not pd.isnull(row.get('Volume')) else None\n",
    "        )\n",
    "        cursor.execute(insert_query, data_tuple)\n",
    "    \n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Data imported successfully into the MySQL database.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script that inserts News DIRECTLY into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for 2024-01-01 to 2024-01-07 for PN\n",
      "Fetching news for 2024-01-08 to 2024-01-14 for PN\n",
      "Fetching news for 2024-01-15 to 2024-01-21 for PN\n",
      "Fetching news for 2024-01-22 to 2024-01-28 for PN\n",
      "Fetching news for 2024-01-29 to 2024-02-04 for PN\n",
      "Fetching news for 2024-02-05 to 2024-02-11 for PN\n",
      "Fetching news for 2024-02-12 to 2024-02-18 for PN\n",
      "Fetching news for 2024-02-19 to 2024-02-25 for PN\n",
      "Fetching news for 2024-02-26 to 2024-03-03 for PN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m             success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Exit retry loop on non-429 errors\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     current_start \u001b[38;5;241m=\u001b[39m current_end \u001b[38;5;241m+\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtimedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Helps avoid rapid-fire requests\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Convert collected news data to DataFrame\u001b[39;00m\n\u001b[1;32m     88\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_news)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from credentials import API_KEYcred, ipCred, usernameCred, passwordCred, databaseCred\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "API_KEY = API_KEYcred\n",
    "BASE_URL = \"https://finnhub.io/api/v1/company-news\"\n",
    "\n",
    "# Replace with any stock ticker\n",
    "ticker = \"PN\"\n",
    "\n",
    "# Database Configuration\n",
    "db_config = {\n",
    "    'host': ipCred,  # Change this if needed\n",
    "    'user': usernameCred,\n",
    "    'password': passwordCred,\n",
    "    'database': databaseCred\n",
    "}\n",
    "\n",
    "# Define the start date and end date (today)\n",
    "start_date = datetime.date(2024, 1, 1)\n",
    "end_date = datetime.date.today()\n",
    "\n",
    "all_news = []\n",
    "current_start = start_date\n",
    "\n",
    "# Maximum number of retries for a 429 error (rate limit)\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "# Establish database connection\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table if not exists\n",
    "create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {ticker}_news (\n",
    "        news_id BIGINT PRIMARY KEY,\n",
    "        date_time DATE,\n",
    "        headline TEXT,\n",
    "        related VARCHAR(10),\n",
    "        source_ VARCHAR(255),\n",
    "        summary TEXT,\n",
    "        sentiment DOUBLE\n",
    "    );\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "while current_start <= end_date:\n",
    "    current_end = current_start + datetime.timedelta(days=6)\n",
    "    if current_end > end_date:\n",
    "        current_end = end_date\n",
    "\n",
    "    params = {\n",
    "        \"symbol\": ticker,\n",
    "        \"from\": current_start.strftime(\"%Y-%m-%d\"),\n",
    "        \"to\": current_end.strftime(\"%Y-%m-%d\"),\n",
    "        \"token\": API_KEY\n",
    "    }\n",
    "\n",
    "    print(f\"Fetching news for {params['from']} to {params['to']} for {ticker}\")\n",
    "\n",
    "    retries = 0\n",
    "    success = False\n",
    "    while not success and retries < MAX_RETRIES:\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        if response.status_code == 200:\n",
    "            news_items = response.json()\n",
    "            if news_items:\n",
    "                all_news.extend(news_items)\n",
    "            success = True\n",
    "        elif response.status_code == 429:\n",
    "            retries += 1\n",
    "            wait_time = 2 ** retries\n",
    "            print(f\"Rate limit reached. Retrying in {wait_time} seconds (attempt {retries}/{MAX_RETRIES})\")\n",
    "            time.sleep(wait_time)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} for range {params['from']} to {params['to']}\")\n",
    "            success = True  # Exit retry loop on non-429 errors\n",
    "\n",
    "    current_start = current_end + datetime.timedelta(days=1)\n",
    "    time.sleep(1)  # Helps avoid rapid-fire requests\n",
    "\n",
    "# Convert collected news data to DataFrame\n",
    "df = pd.DataFrame(all_news)\n",
    "\n",
    "# Helper function to safely convert Unix timestamp to \"YYYY-MM-DD\"\n",
    "def safe_convert(ts):\n",
    "    try:\n",
    "        ts_val = int(ts)\n",
    "        if ts_val <= 0:\n",
    "            return None\n",
    "        return datetime.datetime.fromtimestamp(ts_val).date()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# If the 'datetime' column exists, convert timestamps\n",
    "if not df.empty and 'datetime' in df.columns:\n",
    "    df['datetime'] = df['datetime'].apply(safe_convert)\n",
    "\n",
    "# Insert data into MySQL database\n",
    "insert_query = f\"\"\"\n",
    "    INSERT INTO {ticker}_news (news_id, date_time, headline, related, source_, summary)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    ON DUPLICATE KEY UPDATE \n",
    "        date_time = VALUES(date_time),\n",
    "        headline = VALUES(headline),\n",
    "        related = VALUES(related),\n",
    "        source_ = VALUES(source_),\n",
    "        summary = VALUES(summary);\n",
    "\"\"\"\n",
    "\n",
    "rows_inserted = 0\n",
    "for _, row in df.iterrows():\n",
    "    if pd.isna(row.get('id')) or pd.isna(row.get('datetime')):  # Skip if no ID or Date\n",
    "        continue\n",
    "    data = (\n",
    "        int(row['id']), row['datetime'], row.get('headline', ''),\n",
    "        row.get('related', ''), row.get('source', ''), row.get('summary', '')\n",
    "    )\n",
    "    cursor.execute(insert_query, data)\n",
    "    rows_inserted += 1\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {rows_inserted} news articles into {ticker}_news table.\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
